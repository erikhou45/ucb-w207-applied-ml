{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest, Decision Tree and Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook for testing random forest, decison tree and neural network on the dataset we are given for the W207 final project. Currently only random forest and decision tree are included. Will work on Nerual Network in the week of 7/15.\n",
    "\n",
    "In addition, this notebook included some additional notes on classifying the dataset using multinomial naive bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the packages needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data, shuffle and split it into training and testing sets in 70%, 30% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('forest-cover-type-prediction/train.csv', 'rt') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = np.genfromtxt('forest-cover-type-prediction/train.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = my_data[1:, 1:my_data.shape[1]-1]  # avoid getting headers and ID column\n",
    "labels = my_data[1:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the training data\n",
    "np.random.seed(0)\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(data.shape[0]))\n",
    "\n",
    "shuffled_data = data[shuffle]\n",
    "shuffled_labels = labels[shuffle]\n",
    "shuffled_labels = shuffled_labels - 1\n",
    "\n",
    "# split the data to 60% train, 20% dev and 20% test\n",
    "num_train = int(shuffled_data.shape[0]*0.6)\n",
    "num_dev = int(shuffled_data.shape[0]*0.8)\n",
    "\n",
    "train_data, train_labels = shuffled_data[:num_train], shuffled_labels[:num_train]\n",
    "dev_data, dev_labels = shuffled_data[num_train:num_dev], shuffled_labels[num_train:num_dev]\n",
    "test_data, test_labels = shuffled_data[num_dev:], shuffled_labels[num_dev:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest, Decision Tree and Adaboost\n",
    "\n",
    "Without any feature engineering and optimization, use the above three classifier on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (a decision tree): 0.7751322751322751\n",
      "Accuracy (a random forest): 0.8465608465608465\n",
      "Accuracy (adaboost with decision trees): 0.41898148148148145\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", random_state=0)\n",
    "dt.fit(train_data, train_labels)\n",
    "\n",
    "print('Accuracy (a decision tree):', dt.score(dev_data, dev_labels))\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(train_data, train_labels)\n",
    "\n",
    "print('Accuracy (a random forest):', rfc.score(dev_data, dev_labels))\n",
    "\n",
    "abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "abc.fit(train_data, train_labels)\n",
    "print('Accuracy (adaboost with decision trees):', abc.score(dev_data, dev_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the box, random forest seems to have really good accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Considerations\n",
    "\n",
    "Though\n",
    "\n",
    "\n",
    "1. How to deal with the continuous variables:\n",
    "    * bucketing by percentiles\n",
    "    * bucketing by int(np.log())\n",
    "    * bucketing by fixed width buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bucket_data_percentile(data, bucket):\n",
    "    '''This function transform continuous data into discrete data by putting them into buckets of base on percentile'''\n",
    "\n",
    "    lower_bound = np.min(data)\n",
    "    bucketed_data = np.empty((data.shape[0],0))\n",
    "\n",
    "    for i in range(1, bucket+1):\n",
    "#         print(i, bucket)\n",
    "        upper_bound = np.percentile(data, 100*i/bucket)\n",
    "#         print(lower_bound, upper_bound)\n",
    "        col = i * (data >= lower_bound) * (data < upper_bound)\n",
    "        bucketed_data = np.hstack((bucketed_data,col.reshape(data.shape[0],1)))\n",
    "        lower_bound = upper_bound\n",
    "\n",
    "    output_data = np.sum(bucketed_data, axis=1)\n",
    "#     print(output_data)\n",
    "\n",
    "    return output_data.reshape(output_data.size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_data_con_interval(data, bucket):\n",
    "    '''This function transform continuous data into discrete data by putting them into buckets of constant intervals'''\n",
    "\n",
    "    lower_bound = np.min(data)\n",
    "    bucketed_data = np.empty((data.shape[0],0))\n",
    "\n",
    "    for i in range(1, bucket+1):\n",
    "        \n",
    "        upper_bound = lower_bound+(np.max(data) - np.min(data))/bucket\n",
    "        col = i * (data >= lower_bound) * (data < upper_bound)\n",
    "        bucketed_data = np.hstack((bucketed_data,col.reshape(data.shape[0],1)))\n",
    "        lower_bound = upper_bound\n",
    "\n",
    "    output_data = np.sum(bucketed_data, axis=1)\n",
    "\n",
    "    return output_data.reshape(output_data.size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_data_log(data, bucket):\n",
    "    return np.log(np.absolute(data)+1).astype('int').reshape(data.size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_data(data, cols, bucket_method, bucket_num):\n",
    "    transformed_data = np.empty((data.shape[0], 0))\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "#         print(i)\n",
    "        if i not in cols:\n",
    "            transformed_data = np.hstack((transformed_data, data[:,i].reshape(data.shape[0],1)))\n",
    "        else:\n",
    "            transformed_data = np.hstack((transformed_data, bucket_method(data[:,i],bucket_num)))\n",
    "    \n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is: 1.0\n",
      "--------------------------------------------------\n",
      "alpha = 0.0001, Accuracy =  0.6027966742252456\n",
      "alpha = 0.001, Accuracy =  0.6027966742252456\n",
      "alpha = 0.01, Accuracy =  0.6027966742252456\n",
      "alpha = 0.1, Accuracy =  0.6027966742252456\n",
      "alpha = 0.5, Accuracy =  0.6030801209372638\n",
      "alpha = 1.0, Accuracy =  0.6033635676492819\n",
      "alpha = 2.0, Accuracy =  0.6030801209372638\n",
      "alpha = 10.0, Accuracy =  0.6027021919879063\n",
      "alpha = 20.0, Accuracy =  0.600151171579743\n",
      "alpha = 30.0, Accuracy =  0.5971277399848829\n"
     ]
    }
   ],
   "source": [
    "discrete_train_data = bucket_data(train_data, range(10), bucket_data_con_interval, 7)\n",
    "# discrete_train_data = bucket_data(train_data, range(10), bucket_data_log, None)\n",
    "\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "# mnb.fit(discrete_train_data, train_labels)\n",
    "\n",
    "alphas = {'alpha': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 20.0, 30.0]}\n",
    "\n",
    "grid_search = GridSearchCV(mnb, alphas, cv=5)\n",
    "    \n",
    "# fit the data\n",
    "grid_search.fit(discrete_train_data, train_labels)\n",
    "\n",
    "# report the results \n",
    "print('The best alpha is:', grid_search.best_params_['alpha'])\n",
    "print('-'*50)\n",
    "\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "#     if params['alpha'] == 0 or params['alpha'] == grid_search.best_params_['alpha']:\n",
    "    print('alpha =', '%s,' %params['alpha'], 'Accuracy = ', mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6098828420256992"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the fit to predict on training data\n",
    "\n",
    "discrete_train_data = bucket_data(train_data, range(10), bucket_data_con_interval, 7)\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(discrete_train_data, train_labels)\n",
    "\n",
    "predictions = mnb.predict(discrete_train_data)\n",
    "\n",
    "metrics.accuracy_score(train_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The classifier is underfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to add the interaction of two features as a new feature\n",
    "\n",
    "interactions = train_data[:,0] * train_data[:,5]\n",
    "interactions = interactions.reshape(train_data.shape[0],1)\n",
    "expanded_train_data = np.hstack((train_data, interactions))\n",
    "# print(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6152683295540439"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test fit\n",
    "\n",
    "indices = [i for i in range(10)] + [expanded_train_data.shape[1]-1]\n",
    "# print(indices)\n",
    "discrete_train_data = bucket_data(expanded_train_data, indices, bucket_data_percentile, 7)\n",
    "\n",
    "# print(discrete_train_data[:, -1])\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(discrete_train_data, train_labels)\n",
    "\n",
    "predictions = mnb.predict(discrete_train_data)\n",
    "\n",
    "metrics.accuracy_score(train_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0.,     0.,     0.,     0.,     0., 10584.,     0.,     0.,\n",
       "            0.,     0.]),\n",
       " array([0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3, 1.4, 1.5]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "soil_sum = np.sum(train_data[:, 14:], axis = 1)\n",
    "\n",
    "plt.hist(soil_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Only one soil is one per example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network (will work on this in the week of 7/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "\n",
    "import theano \n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "print(theano.config.device) # We're using CPUs (for now)\n",
    "print(theano.config.floatX) # Should be 64 bit for CPUs\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def binarizeY(data):\n",
    "    binarized_data = np.zeros((data.size,7))\n",
    "    for j in range(0,data.size):\n",
    "        feature = data[j:j+1]\n",
    "        i = feature.astype(np.int64)\n",
    "        binarized_data[j,i]=1\n",
    "    return binarized_data\n",
    "train_labels_b = binarizeY(train_labels)\n",
    "dev_labels_b = binarizeY(dev_labels)\n",
    "numClasses = train_labels_b[1].size\n",
    "# print('Classes = %d' %(numClasses))\n",
    "\n",
    "# print(train_labels_b[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
